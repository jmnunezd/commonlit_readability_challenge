{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/Desktop/Udacity_Project/tools.py:16: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tools import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collect-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_creative_features(dataframe):\n",
    "    \"\"\"\n",
    "    This function applies the tools.py functions I wrote into every excerpt.\n",
    "    \"\"\"\n",
    "    dataframe['num_punct_marks'] = dataframe['excerpt'].apply(num_punct_marks)\n",
    "    dataframe['num_uniq_words'] = dataframe['excerpt'].apply(num_unique_words)\n",
    "    dataframe['avg_word_len'] = dataframe['excerpt'].apply(avg_word_len)\n",
    "    dataframe['rarity'] = dataframe['excerpt'].apply(rarity)\n",
    "\n",
    "\n",
    "def add_clasic_test(dataframe):\n",
    "    \"\"\"\n",
    "    This function applies all the publicly available readability test into every excerpt.\n",
    "    \"\"\"\n",
    "    clasical_complexity_tests = {'fre_test': textstat.flesch_reading_ease,\n",
    "                                 'fkg_test': textstat.flesch_kincaid_grade,\n",
    "                                 'gf_test': textstat.gunning_fog,\n",
    "                                 'si_test': textstat.smog_index,\n",
    "                                 'dcrs_test': textstat.dale_chall_readability_score}\n",
    "\n",
    "    # Creating text complexity feature for every test:\n",
    "    for test in clasical_complexity_tests.keys():\n",
    "        test_func = clasical_complexity_tests[test]\n",
    "\n",
    "        dataframe[test] = dataframe['excerpt'].apply(lambda value: test_func(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-magic",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "* After a lot of iteration and testing, this is the best model I have found so far.\n",
    "* The main improvement here is adding the TF-IDF (Term frequency - Inverse Frequency) pre-procesing to the mix.\n",
    "* Also I tweak the hyperparameters of best models to find the best combination.\n",
    "\n",
    "#### I'm going to import the train.csv again to mantain order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secondary-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', usecols=['id', 'excerpt', 'target'])\n",
    "validation = pd.read_csv('validation.csv', usecols=['id', 'excerpt', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appointed-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_creative_features(train)\n",
    "add_clasic_test(train)\n",
    "\n",
    "add_creative_features(validation)\n",
    "add_clasic_test(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor vs target... R2: 0.51\n",
      "MLPRegressor  vs target... MAE: 0.594\n",
      "MLPRegressor  vs target... RMSE: 0.712 \n",
      "\n",
      "SVR vs target... R2: 0.504\n",
      "SVR  vs target... MAE: 0.59\n",
      "SVR  vs target... RMSE: 0.717 \n",
      "\n",
      "Ridge vs target... R2: 0.542\n",
      "Ridge  vs target... MAE: 0.577\n",
      "Ridge  vs target... RMSE: 0.689 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 0: tfidf alone\n",
    "X_train = train['excerpt']\n",
    "y_train = train['target']\n",
    "\n",
    "X_val = validation['excerpt']\n",
    "y_val = validation['target']\n",
    "\n",
    "models = {'MLPRegressor': MLPRegressor(),\n",
    "          'SVR': SVR(kernel = 'rbf'),\n",
    "          'Ridge': Ridge()}\n",
    "\n",
    "for model, regr in models.items():\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('rgr', regr)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # validation predictions\n",
    "    preds = pipeline.predict(X_val)\n",
    "    \n",
    "    # metrics\n",
    "    r2 = round(pipeline.score(X_val, y_val), 3)\n",
    "    mae = round(mean_absolute_error(y_val, preds), 3)\n",
    "    rmse = round(math.sqrt(mean_squared_error(y_val, preds)), 3)\n",
    "    \n",
    "    # Printing results\n",
    "    print(f'{model} vs target... R2: {r2}')\n",
    "    print(f'{model}  vs target... MAE: {mae}')\n",
    "    print(f'{model}  vs target... RMSE: {rmse}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "institutional-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor vs target... R2: 0.529\n",
      "MLPRegressor  vs target... MAE: 0.584\n",
      "MLPRegressor  vs target... RMSE: 0.698 \n",
      "\n",
      "SVR vs target... R2: 0.546\n",
      "SVR  vs target... MAE: 0.574\n",
      "SVR  vs target... RMSE: 0.686 \n",
      "\n",
      "Ridge vs target... R2: 0.508\n",
      "Ridge  vs target... MAE: 0.6\n",
      "Ridge  vs target... RMSE: 0.714 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: joining previous output as input + normal variables\n",
    "train['ridge_preds'] = pipeline.predict(X_train)\n",
    "validation['ridge_preds'] = pipeline.predict(X_val)\n",
    "\n",
    "variables = ['rarity', 'avg_word_len', 'fre_test', 'dcrs_test', 'ridge_preds']\n",
    "\n",
    "# Let's iterate over every candidate model, train it and compare results\n",
    "\n",
    "models = {'MLPRegressor': MLPRegressor(max_iter=1000, learning_rate='adaptive', early_stopping=True),\n",
    "          'SVR': SVR(kernel='rbf', C=10),\n",
    "          'Ridge': Ridge(alpha=2)}\n",
    "\n",
    "for model, regressor in models.items():\n",
    "    # training:\n",
    "    X_train = train[variables].values\n",
    "    \n",
    "    # fitting model\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # checking the model results in the validation set\n",
    "    X_test = validation[variables].values\n",
    "    X_test_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # metrics\n",
    "    r2 = round(regressor.score(X_test, y_val), 3)\n",
    "    mae = round(mean_absolute_error(y_val, X_test_pred), 3)\n",
    "    rmse = round(math.sqrt(mean_squared_error(y_val, X_test_pred)), 3)\n",
    "    \n",
    "    # Printing results\n",
    "    print(f'{model} vs target... R2: {r2}')\n",
    "    print(f'{model}  vs target... MAE: {mae}')\n",
    "    print(f'{model}  vs target... RMSE: {rmse}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-arthur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
